{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30492,
     "status": "ok",
     "timestamp": 1576739156296,
     "user": {
      "displayName": "Runner Hong",
      "photoUrl": "",
      "userId": "08880490206113528542"
     },
     "user_tz": -480
    },
    "id": "o5TvGTNPQ0t4",
    "outputId": "7de9dc97-913a-4a37-8ae9-5f8bb37d1b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "import os\n",
    "os.chdir('/content/gdrive/My Drive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8US7DzGWmG8"
   },
   "outputs": [],
   "source": [
    "# os.chdir('/content/gdrive/My Drive/testpy')\n",
    "\n",
    "# %run __init__.py\n",
    "# # Test().test()\n",
    "# # print(dir(Test))\n",
    "# # Test().test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoaSo9QsRW5T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import os\n",
    "import sys\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zH7_CeDpdcq6"
   },
   "outputs": [],
   "source": [
    "# config.py\n",
    "import os.path\n",
    "\n",
    "# gets home dir cross platform\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "\n",
    "# for making bounding boxes pretty\n",
    "COLORS = ((255, 0, 0, 128), (0, 255, 0, 128), (0, 0, 255, 128),\n",
    "          (0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128))\n",
    "\n",
    "MEANS = (104, 117, 123)\n",
    "\n",
    "# SSD300 CONFIGS\n",
    "voc = {\n",
    "    'num_classes': 21,\n",
    "    'lr_steps': (80000, 100000, 120000),\n",
    "    'max_iter': 120000,\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'min_dim': 300,\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': True,\n",
    "    'name': 'VOC',\n",
    "}\n",
    "\n",
    "coco = {\n",
    "    'num_classes': 201,\n",
    "    'lr_steps': (280000, 360000, 400000),\n",
    "    'max_iter': 400000,\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'min_dim': 300,\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [21, 45, 99, 153, 207, 261],\n",
    "    'max_sizes': [45, 99, 153, 207, 261, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': True,\n",
    "    'name': 'COCO',\n",
    "}\n",
    "\n",
    "_sixray = {\n",
    "    'num_classes': 3,\n",
    "    'lr_steps': (8000, 10000, 12000),\n",
    "    'max_iter': 12000,\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'min_dim': 300,\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': True,\n",
    "    'name': 'VOC',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnYo1kXoRafk"
   },
   "outputs": [],
   "source": [
    "SIXray_CLASSES = (\n",
    "    '带电芯充电宝', '不带电芯充电宝'\n",
    ")\n",
    "# note: if you used our download scripts, this should be right\n",
    "SIXray_ROOT = \"/content/gdrive/My Drive/all/\"\n",
    "\n",
    "\n",
    "class SIXrayAnnotationTransform(object):\n",
    "    \"\"\"Transforms a VOC annotation into a Tensor of bbox coords and label index\n",
    "    Initilized with a dictionary lookup of classnames to indexes\n",
    "\n",
    "    Arguments:\n",
    "        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n",
    "            (default: alphabetic indexing of VOC's 20 classes)\n",
    "        keep_difficult (bool, optional): keep difficult instances or not\n",
    "            (default: False)\n",
    "        height (int): height\n",
    "        width (int): width\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_to_ind=None, keep_difficult=False):\n",
    "        self.class_to_ind = class_to_ind or dict(\n",
    "            # zip(SIXray_CLASSES, range(len(SIXray_CLASSES))))\n",
    "            zip(SIXray_CLASSES, range(len(SIXray_CLASSES))))\n",
    "        self.keep_difficult = keep_difficult\n",
    "        # 添加的记录所有小类总数\n",
    "        self.type_dict = {}\n",
    "        # 记录大类数量\n",
    "        self.type_sum_dict = {}\n",
    "\n",
    "    def __call__(self, target, width, height, idx):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            target (annotation) : the target annotation to be made usable\n",
    "                will be an ET.Element\n",
    "            it has been changed to the path of annotation-2019-07-10\n",
    "        Returns:\n",
    "            a list containing lists of bounding boxes  [bbox coords, class name]\n",
    "        \"\"\"\n",
    "        # print (idx)\n",
    "\n",
    "        ###读取所有类\n",
    "        # class_path = \"C:/Users/石玉峰/PycharmProjects/untitled1/package2/class.xml\"\n",
    "        '''\n",
    "        class_path = SIXray_ROOT + \"class.xml\"\n",
    "        tree = ET.parse(class_path)\n",
    "        class_root = tree.getroot()\n",
    "        for child in class_root:\n",
    "            one_dict = {}\n",
    "            type_name = None\n",
    "            find_name = 0\n",
    "            for grandson in child:\n",
    "                if find_name == 0:\n",
    "                    find_name = 1\n",
    "                    type_name = grandson.text\n",
    "                else:\n",
    "                    one_dict[grandson.text] = 0\n",
    "            self.type_dict[type_name] = one_dict\n",
    "        #print(self.type_dict)\n",
    "        '''\n",
    "        # 遍历Annotation\n",
    "        # root_annotation = '/media/dsg3/datasets/Xray20190704/Annotation/'\n",
    "        res = []\n",
    "        with open(target, \"r\", encoding='utf-8') as f1:\n",
    "            dataread = f1.readlines()\n",
    "        for annotation in dataread:\n",
    "            bndbox = []\n",
    "            temp = annotation.split()\n",
    "            name = temp[1]\n",
    "            # 只读两类\n",
    "            if name != '带电芯充电宝' and name != '不带电芯充电宝':\n",
    "                continue\n",
    "            xmin = int(temp[2]) / width\n",
    "            # 只读取V视角的\n",
    "            if xmin > 1:\n",
    "                continue\n",
    "            if xmin < 0:\n",
    "                xmin = 0\n",
    "            ymin = int(temp[3]) / height\n",
    "            if ymin < 0:\n",
    "                ymin = 0\n",
    "            xmax = int(temp[4]) / width\n",
    "            if xmax > 1:  # 是这么个意思吧？\n",
    "                xmax = 1\n",
    "            ymax = int(temp[5]) / height\n",
    "            if ymax > 1:\n",
    "                ymax = 1\n",
    "            bndbox.append(xmin)\n",
    "            bndbox.append(ymin)\n",
    "            bndbox.append(xmax)\n",
    "            bndbox.append(ymax)\n",
    "            label_idx = self.class_to_ind[name]\n",
    "            # label_idx = name\n",
    "            bndbox.append(label_idx)\n",
    "            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n",
    "        if len(res) == 0:\n",
    "            return [[0, 0, 0, 0, 3]]\n",
    "        return res\n",
    "\n",
    "\n",
    "class SIXrayDetection(data.Dataset):\n",
    "    \"\"\"VOC Detection Dataset Object\n",
    "\n",
    "    input is image, target is annotation\n",
    "\n",
    "    Arguments:\n",
    "        root (string): filepath to VOCdevkit folder.\n",
    "        image_set (string): imageset to use (eg. 'train', 'val', 'test')\n",
    "        transform (callable, optional): transformation to perform on the\n",
    "            input image\n",
    "        target_transform (callable, optional): transformation to perform on the\n",
    "            target `annotation`\n",
    "            (eg: take in caption string, return tensor of word indices)\n",
    "        dataset_name (string, optional): which dataset to load\n",
    "            (default: 'VOC2007')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root,\n",
    "                 image_sets,\n",
    "                 transform=None, target_transform=SIXrayAnnotationTransform(),\n",
    "                 # dataset_name='SIXray'):\n",
    "                 dataset_name='SIXray', phase='train'):\n",
    "        # self.root = root\n",
    "        self.name = 'sixray'\n",
    "        self.root = SIXray_ROOT\n",
    "        self.image_set = image_sets\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        # self.name = dataset_name\n",
    "        #self.name = 'Xray0723_bat_core_coreless'\n",
    "        self._annopath = osp.join('%s' % self.root, 'Annotation', '%s.txt')\n",
    "        self._imgpath = osp.join('%s' % self.root, 'Image', '%s.jpg')\n",
    "        # self._annopath = osp.join('%s' % self.root, 'Annotation', '%s.xml')\n",
    "        # self._annopath = osp.join('%s' % self.root, 'Anno_core_coreless_battery_sub_2000_500', '%s.txt')\n",
    "        # self._imgpath = osp.join('%s' % self.root, 'Image', '%s.jpg')\n",
    "        # self._imgpath = osp.join('%s' % self.root, 'cut_Image_core_coreless_battery_sub_2000_500', '%s.TIFF')\n",
    "        ###这尼玛还有小写的tiff？\n",
    "        # self._imgpath1 = osp.join('%s' % self.root, 'cut_Image_core_coreless_battery_sub_2000_500', '%s.tiff')\n",
    "        # self._imgpath_jpg = osp.join('%s' % self.root, 'cut_Image_core_coreless_battery_sub_2000_500', '%s.jpg')\n",
    "        self.ids = list()\n",
    "\n",
    "        listdir = os.listdir(osp.join('%s' % self.root, 'Annotation'))\n",
    "        # testdir = listdir[0:6000:5]\n",
    "        testdir = listdir[0:6000:10]\n",
    "        print(testdir)\n",
    "\n",
    "        # with open(self.image_set, 'r') as f:\n",
    "        #     lines = f.readlines()\n",
    "        #     for line in lines:\n",
    "        #         self.ids.append(line.strip('\\n'))\n",
    "        if phase == 'train':\n",
    "          traindir = set(listdir).difference(set(testdir))\n",
    "          # traindir = list(traindir)[0:1200]\n",
    "          for name in traindir:\n",
    "              self.ids.append(osp.splitext(name)[0])\n",
    "        else:\n",
    "          for name in testdir:\n",
    "              self.ids.append(osp.splitext(name)[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im, gt, h, w, og_im = self.pull_item(index)\n",
    "\n",
    "        return im, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        # target = ET.parse(self._annopath % img_id).getroot()\n",
    "\n",
    "        target = self._annopath % img_id  # 注释目录\n",
    "        # print(target)\n",
    "        # print(self._imgpath % img_id)\n",
    "        img = cv2.imread(self._imgpath % img_id)\n",
    "        # if img is None:\n",
    "        #     img = cv2.imread(self._imgpath1 % img_id)\n",
    "        # if img is None:\n",
    "        #     img = cv2.imread(self._imgpath_jpg % img_id)\n",
    "\n",
    "        if img is None:\n",
    "            print('\\nwrong\\n')\n",
    "            # print(self._imgpath_jpg % img_id)\n",
    "\n",
    "        # print()\n",
    "        height, width, channels = img.shape\n",
    "        # print(\"height: \" + str(height) + \" ; width : \" + str(width) + \" ; channels \" + str(channels) )\n",
    "        og_img = img\n",
    "\n",
    "        # print (img_id)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target, width, height, img_id)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            target = np.array(target)\n",
    "            # print(target)\n",
    "            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n",
    "            # to rgb\n",
    "            img = img[:, :, (2, 1, 0)]\n",
    "            # img = img.transpose(a2, 0, a1)\n",
    "            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
    "        return torch.from_numpy(img).permute(2, 0, 1), target, height, width, og_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEx6ErgCRqeE"
   },
   "outputs": [],
   "source": [
    "\n",
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on\n",
    "                                 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        targets.append(torch.FloatTensor(sample[1]))\n",
    "    return torch.stack(imgs, 0), targets\n",
    "\n",
    "\n",
    "def base_transform(image, size, mean):\n",
    "    x = cv2.resize(image, (size, size)).astype(np.float32)\n",
    "    x -= mean\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "\n",
    "class BaseTransform:\n",
    "    def __init__(self, size, mean):\n",
    "        self.size = size\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        return base_transform(image, self.size, self.mean), boxes, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhIQJQM1SpyC"
   },
   "outputs": [],
   "source": [
    "\n",
    "def point_form(boxes):\n",
    "    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n",
    "    representation for comparison to point form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) center-size default boxes from priorbox layers.\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n",
    "\n",
    "\n",
    "def center_size(boxes):\n",
    "    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n",
    "    representation for comparison to center-size form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) point_form boxes\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n",
    "                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n",
    "\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = box_a.size(0)\n",
    "    B = box_b.size(0)\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "\n",
    "def jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
    "    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n",
    "    overlap, encode the bounding boxes, then return the matched indices\n",
    "    corresponding to both confidence and location preds.\n",
    "    Args:\n",
    "        threshold: (float) The overlap threshold used when mathing boxes.\n",
    "        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n",
    "        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n",
    "        variances: (tensor) Variances corresponding to each prior coord,\n",
    "            Shape: [num_priors, 4].\n",
    "        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n",
    "        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n",
    "        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n",
    "        idx: (int) current batch index\n",
    "    Return:\n",
    "        The matched indices corresponding to 1)location and 2)confidence preds.\n",
    "    \"\"\"\n",
    "    # jaccard index\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        point_form(priors)\n",
    "    )\n",
    "    # (Bipartite Matching)\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
    "    # [1,num_priors] best ground truth for each prior\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n",
    "    # TODO refactor: index  best_prior_idx with long tensor\n",
    "    # ensure every gt matches with its prior of max overlap\n",
    "    for j in range(best_prior_idx.size(0)):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n",
    "    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n",
    "    conf[best_truth_overlap < threshold] = 0  # label as background\n",
    "    loc = encode(matches, priors, variances)\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
    "\n",
    "\n",
    "def encode(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
    "    we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh) / variances[1]\n",
    "    # return target for smooth_l1_loss\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n",
    "\n",
    "\n",
    "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    \"\"\"Utility function for computing log_sum_exp while determining\n",
    "    This will be used to determine unaveraged confidence loss across\n",
    "    all examples in a batch.\n",
    "    Args:\n",
    "        x (Variable(tensor)): conf_preds from conf layers\n",
    "    \"\"\"\n",
    "    x_max = x.data.max()\n",
    "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
    "\n",
    "\n",
    "# Original author: Francisco Massa:\n",
    "# https://github.com/fmassa/object-detection.torch\n",
    "# Ported to PyTorch by Max deGroot (02/01/2017)\n",
    "def nms(boxes, scores, overlap=0.5, top_k=200):\n",
    "    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object.\n",
    "    Args:\n",
    "        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n",
    "        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n",
    "        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n",
    "        top_k: (int) The Maximum number of box preds to consider.\n",
    "    Return:\n",
    "        The indices of the kept boxes with respect to num_priors.\n",
    "    \"\"\"\n",
    "\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0:\n",
    "        return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    # I = I[v >= 0.01]\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    # keep = torch.Tensor()\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        # keep.append(i)\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1:\n",
    "            break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PB_u4kurS7GP"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class Detect(Function):\n",
    "    \"\"\"At test time, Detect is the final layer of SSD.  Decode location preds,\n",
    "    apply non-maximum suppression to location predictions based on conf\n",
    "    scores and threshold to a top_k number of output predictions for both\n",
    "    confidence score and locations.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):\n",
    "        self.num_classes = num_classes\n",
    "        self.background_label = bkg_label\n",
    "        self.top_k = top_k\n",
    "        # Parameters used in nms.\n",
    "        self.nms_thresh = nms_thresh\n",
    "        if nms_thresh <= 0:\n",
    "            raise ValueError('nms_threshold must be non negative.')\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.variance = cfg['variance']\n",
    "\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loc_data: (tensor) Loc preds from loc layers\n",
    "                Shape: [batch,num_priors*4]\n",
    "            conf_data: (tensor) Shape: Conf preds from conf layers\n",
    "                Shape: [batch*num_priors,num_classes]\n",
    "            prior_data: (tensor) Prior boxes and variances from priorbox layers\n",
    "                Shape: [1,num_priors,4]\n",
    "        \"\"\"\n",
    "        loc_data = args[0]\n",
    "        conf_data = args[1]\n",
    "        prior_data = args[2]\n",
    "        num = loc_data.size(0)  # batch size\n",
    "        num_priors = prior_data.size(0)\n",
    "        output = torch.zeros(num, self.num_classes, self.top_k, 5)\n",
    "        conf_preds = conf_data.view(num, num_priors,\n",
    "                                    self.num_classes).transpose(2, 1)\n",
    "\n",
    "        # Decode predictions into bboxes.\n",
    "        for i in range(num):\n",
    "            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n",
    "            # For each class, perform nms\n",
    "            conf_scores = conf_preds[i].clone()\n",
    "\n",
    "            for cl in range(1, self.num_classes):\n",
    "                c_mask = conf_scores[cl].gt(self.conf_thresh)\n",
    "                scores = conf_scores[cl][c_mask]\n",
    "                if scores.size(0) == 0:\n",
    "                    continue\n",
    "                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n",
    "                boxes = decoded_boxes[l_mask].view(-1, 4)\n",
    "                # idx of highest scoring and non-overlapping boxes per class\n",
    "                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n",
    "                output[i, cl, :count] = \\\n",
    "                    torch.cat((scores[ids[:count]].unsqueeze(1),\n",
    "                               boxes[ids[:count]]), 1)\n",
    "        flt = output.contiguous().view(num, -1, 5)\n",
    "        _, idx = flt[:, :, 0].sort(1, descending=True)\n",
    "        _, rank = idx.sort(1)\n",
    "        flt[(rank < self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NgxYcfqaTDNX"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from math import sqrt as sqrt\n",
    "from itertools import product as product\n",
    "import torch\n",
    "\n",
    "\n",
    "class PriorBox(object):\n",
    "    \"\"\"Compute priorbox coordinates in center-offset form for each source\n",
    "    feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.image_size = cfg['min_dim']\n",
    "        # number of priors for feature map location (either 4 or 6)\n",
    "        self.num_priors = len(cfg['aspect_ratios'])\n",
    "        self.variance = cfg['variance'] or [0.1]\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.aspect_ratios = cfg['aspect_ratios']\n",
    "        self.clip = cfg['clip']\n",
    "        self.version = cfg['name']\n",
    "        for v in self.variance:\n",
    "            if v <= 0:\n",
    "                raise ValueError('Variances must be greater than 0')\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.image_size / self.steps[k]\n",
    "                # unit center x,y\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: min_size\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: sqrt(s_k * s_(k+1))\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                # rest of aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUaTXOeV9Qct"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbdO3hdlTIn5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self,n_channels, scale):\n",
    "        super(L2Norm,self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.gamma = scale or None\n",
    "        self.eps = 1e-10\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.constant_(self.weight,self.gamma)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n",
    "        #x /= norm\n",
    "        x = torch.div(x,norm)\n",
    "        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0NnscYaTNjE"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "    Objective Loss:\n",
    "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
    "        weighted by α which is set to 1 by cross val.\n",
    "        Args:\n",
    "            c: class confidences,\n",
    "            l: predicted boxes,\n",
    "            g: ground truth boxes\n",
    "            N: number of matched default boxes\n",
    "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, overlap_thresh, prior_for_matching,\n",
    "                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n",
    "                 use_gpu=True):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.num_classes = cfg['num_classes']\n",
    "        self.threshold = overlap_thresh\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching = prior_for_matching\n",
    "        self.do_neg_mining = neg_mining\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        self.variance = cfg['variance']\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"Multibox Loss\n",
    "        Args:\n",
    "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
    "            and prior boxes from SSD net.\n",
    "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
    "                loc shape: torch.size(batch_size,num_priors,4)\n",
    "                priors shape: torch.size(num_priors,4)\n",
    "\n",
    "            targets (tensor): Ground truth boxes and labels for a batch,\n",
    "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
    "        \"\"\"\n",
    "        loc_data, conf_data, priors = predictions\n",
    "        num = loc_data.size(0)\n",
    "        priors = priors[:loc_data.size(1), :]\n",
    "        num_priors = (priors.size(0))\n",
    "        num_classes = self.num_classes\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "            match(self.threshold, truths, defaults, self.variance, labels,\n",
    "                  loc_t, conf_t, idx)\n",
    "        if self.use_gpu:\n",
    "            loc_t = loc_t.cuda()\n",
    "            conf_t = conf_t.cuda()\n",
    "        # wrap targets\n",
    "        loc_t = Variable(loc_t, requires_grad=False)\n",
    "        conf_t = Variable(conf_t, requires_grad=False)\n",
    "\n",
    "        pos = conf_t > 0\n",
    "        num_pos = pos.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Localization Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,4]\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        # size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
    "        # loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        # https://github.com/amdegroot/ssd.pytorch/issues/173\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        loss_c[pos] = 0  # filter out pos boxes for now\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        # https://github.com/amdegroot/ssd.pytorch/issues/173\n",
    "        N = num_pos.data.sum().double()\n",
    "        loss_l = loss_l.double()\n",
    "        loss_c = loss_c.double()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        return loss_l, loss_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PAkBZ3xpbM3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import types\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "def intersect_numpy(box_a, box_b):\n",
    "    \n",
    "    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n",
    "    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n",
    "    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n",
    "    return inter[:, 0] * inter[:, 1]\n",
    "\n",
    "\n",
    "def jaccard_numpy(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n",
    "        box_b: Single bounding box, Shape: [4]\n",
    "    Return:\n",
    "        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n",
    "    \"\"\"\n",
    "    inter = intersect_numpy(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n",
    "    area_b = ((box_b[2]-box_b[0]) *\n",
    "              (box_b[3]-box_b[1]))  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several augmentations together.\n",
    "    Args:\n",
    "        transforms (List[Transform]): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> augmentations.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, boxes=None, labels=None):\n",
    "        for t in self.transforms:\n",
    "            img, boxes, labels = t(img, boxes, labels)\n",
    "        return img, boxes, labels\n",
    "\n",
    "\n",
    "class Lambda(object):\n",
    "    \"\"\"Applies a lambda as a transform.\"\"\"\n",
    "\n",
    "    def __init__(self, lambd):\n",
    "        assert isinstance(lambd, types.LambdaType)\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def __call__(self, img, boxes=None, labels=None):\n",
    "        return self.lambd(img, boxes, labels)\n",
    "\n",
    "\n",
    "class ConvertFromInts(object):\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        return image.astype(np.float32), boxes, labels\n",
    "\n",
    "\n",
    "class SubtractMeans(object):\n",
    "    def __init__(self, mean):\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        image = image.astype(np.float32)\n",
    "        image -= self.mean\n",
    "        return image.astype(np.float32), boxes, labels\n",
    "\n",
    "\n",
    "class ToAbsoluteCoords(object):\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        height, width, channels = image.shape\n",
    "        boxes[:, 0] *= width\n",
    "        boxes[:, 2] *= width\n",
    "        boxes[:, 1] *= height\n",
    "        boxes[:, 3] *= height\n",
    "\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class ToPercentCoords(object):\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        height, width, channels = image.shape\n",
    "        boxes[:, 0] /= width\n",
    "        boxes[:, 2] /= width\n",
    "        boxes[:, 1] /= height\n",
    "        boxes[:, 3] /= height\n",
    "\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size=300):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        image = cv2.resize(image, (self.size,\n",
    "                                 self.size))\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class RandomSaturation(object):\n",
    "    def __init__(self, lower=0.5, upper=1.5):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n",
    "        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        if random.randint(2):\n",
    "            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n",
    "\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class RandomHue(object):\n",
    "    def __init__(self, delta=18.0):\n",
    "        assert delta >= 0.0 and delta <= 360.0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        if random.randint(2):\n",
    "            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n",
    "            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n",
    "            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class RandomLightingNoise(object):\n",
    "    def __init__(self):\n",
    "        self.perms = ((0, 1, 2), (0, 2, 1),\n",
    "                      (1, 0, 2), (1, 2, 0),\n",
    "                      (2, 0, 1), (2, 1, 0))\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        if random.randint(2):\n",
    "            swap = self.perms[random.randint(len(self.perms))]\n",
    "            shuffle = SwapChannels(swap)  # shuffle channels\n",
    "            image = shuffle(image)\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class ConvertColor(object):\n",
    "    def __init__(self, current='BGR', transform='HSV'):\n",
    "        self.transform = transform\n",
    "        self.current = current\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        if self.current == 'BGR' and self.transform == 'HSV':\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        elif self.current == 'HSV' and self.transform == 'BGR':\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class RandomContrast(object):\n",
    "    def __init__(self, lower=0.5, upper=1.5):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n",
    "        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n",
    "\n",
    "    # expects float image\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        if random.randint(2):\n",
    "            alpha = random.uniform(self.lower, self.upper)\n",
    "            image *= alpha\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class RandomBrightness(object):\n",
    "    def __init__(self, delta=32):\n",
    "        assert delta >= 0.0\n",
    "        assert delta <= 255.0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        if random.randint(2):\n",
    "            delta = random.uniform(-self.delta, self.delta)\n",
    "            image += delta\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class ToCV2Image(object):\n",
    "    def __call__(self, tensor, boxes=None, labels=None):\n",
    "        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, cvimage, boxes=None, labels=None):\n",
    "        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n",
    "\n",
    "\n",
    "class RandomSampleCrop(object):\n",
    "    \"\"\"Crop\n",
    "    Arguments:\n",
    "        img (Image): the image being input during training\n",
    "        boxes (Tensor): the original bounding boxes in pt form\n",
    "        labels (Tensor): the class labels for each bbox\n",
    "        mode (float tuple): the min and max jaccard overlaps\n",
    "    Return:\n",
    "        (img, boxes, classes)\n",
    "            img (Image): the cropped image\n",
    "            boxes (Tensor): the adjusted bounding boxes in pt form\n",
    "            labels (Tensor): the class labels for each bbox\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sample_options = (\n",
    "            # using entire original input image\n",
    "            None,\n",
    "            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n",
    "            (0.1, None),\n",
    "            (0.3, None),\n",
    "            (0.7, None),\n",
    "            (0.9, None),\n",
    "            # randomly sample a patch\n",
    "            (None, None),\n",
    "        )\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        height, width, _ = image.shape\n",
    "        while True:\n",
    "            # randomly choose a mode\n",
    "            mode = random.choice(self.sample_options)\n",
    "            if mode is None:\n",
    "                return image, boxes, labels\n",
    "\n",
    "            min_iou, max_iou = mode\n",
    "            if min_iou is None:\n",
    "                min_iou = float('-inf')\n",
    "            if max_iou is None:\n",
    "                max_iou = float('inf')\n",
    "\n",
    "            # max trails (50)\n",
    "            for _ in range(50):\n",
    "                current_image = image\n",
    "\n",
    "                w = random.uniform(0.3 * width, width)\n",
    "                h = random.uniform(0.3 * height, height)\n",
    "\n",
    "                # aspect ratio constraint b/t .5 & 2\n",
    "                if h / w < 0.5 or h / w > 2:\n",
    "                    continue\n",
    "\n",
    "                left = random.uniform(width - w)\n",
    "                top = random.uniform(height - h)\n",
    "\n",
    "                # convert to integer rect x1,y1,x2,y2\n",
    "                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n",
    "\n",
    "                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n",
    "                overlap = jaccard_numpy(boxes, rect)\n",
    "\n",
    "                # is min and max overlap constraint satisfied? if not try again\n",
    "                if overlap.min() < min_iou and max_iou < overlap.max():\n",
    "                    continue\n",
    "\n",
    "                # cut the crop from the image\n",
    "                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n",
    "                                              :]\n",
    "\n",
    "                # keep overlap with gt box IF center in sampled patch\n",
    "                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n",
    "\n",
    "                # mask in all gt boxes that above and to the left of centers\n",
    "                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n",
    "\n",
    "                # mask in all gt boxes that under and to the right of centers\n",
    "                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n",
    "\n",
    "                # mask in that both m1 and m2 are true\n",
    "                mask = m1 * m2\n",
    "\n",
    "                # have any valid boxes? try again if not\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                # take only matching gt boxes\n",
    "                current_boxes = boxes[mask, :].copy()\n",
    "\n",
    "                # take only matching gt labels\n",
    "                current_labels = labels[mask]\n",
    "\n",
    "                # should we use the box left and top corner or the crop's\n",
    "                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n",
    "                                                  rect[:2])\n",
    "                # adjust to crop (by substracting crop's left,top)\n",
    "                current_boxes[:, :2] -= rect[:2]\n",
    "\n",
    "                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n",
    "                                                  rect[2:])\n",
    "                # adjust to crop (by substracting crop's left,top)\n",
    "                current_boxes[:, 2:] -= rect[:2]\n",
    "\n",
    "                return current_image, current_boxes, current_labels\n",
    "\n",
    "\n",
    "class Expand(object):\n",
    "    def __init__(self, mean):\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, image, boxes, labels):\n",
    "        if random.randint(2):\n",
    "            return image, boxes, labels\n",
    "\n",
    "        height, width, depth = image.shape\n",
    "        ratio = random.uniform(1, 4)\n",
    "        left = random.uniform(0, width*ratio - width)\n",
    "        top = random.uniform(0, height*ratio - height)\n",
    "\n",
    "        expand_image = np.zeros(\n",
    "            (int(height*ratio), int(width*ratio), depth),\n",
    "            dtype=image.dtype)\n",
    "        expand_image[:, :, :] = self.mean\n",
    "        expand_image[int(top):int(top + height),\n",
    "                     int(left):int(left + width)] = image\n",
    "        image = expand_image\n",
    "\n",
    "        boxes = boxes.copy()\n",
    "        boxes[:, :2] += (int(left), int(top))\n",
    "        boxes[:, 2:] += (int(left), int(top))\n",
    "\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "class RandomMirror(object):\n",
    "    def __call__(self, image, boxes, classes):\n",
    "        _, width, _ = image.shape\n",
    "        if random.randint(2):\n",
    "            image = image[:, ::-1]\n",
    "            boxes = boxes.copy()\n",
    "            boxes[:, 0::2] = width - boxes[:, 2::-2]\n",
    "        return image, boxes, classes\n",
    "\n",
    "\n",
    "class SwapChannels(object):\n",
    "    \"\"\"Transforms a tensorized image by swapping the channels in the order\n",
    "     specified in the swap tuple.\n",
    "    Args:\n",
    "        swaps (int triple): final order of channels\n",
    "            eg: (2, 1, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, swaps):\n",
    "        self.swaps = swaps\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (Tensor): image tensor to be transformed\n",
    "        Return:\n",
    "            a tensor with channels swapped according to swap\n",
    "        \"\"\"\n",
    "        # if torch.is_tensor(image):\n",
    "        #     image = image.data.cpu().numpy()\n",
    "        # else:\n",
    "        #     image = np.array(image)\n",
    "        image = image[:, :, self.swaps]\n",
    "        return image\n",
    "\n",
    "\n",
    "class PhotometricDistort(object):\n",
    "    def __init__(self):\n",
    "        self.pd = [\n",
    "            RandomContrast(),\n",
    "            ConvertColor(transform='HSV'),\n",
    "            RandomSaturation(),\n",
    "            RandomHue(),\n",
    "            ConvertColor(current='HSV', transform='BGR'),\n",
    "            RandomContrast()\n",
    "        ]\n",
    "        self.rand_brightness = RandomBrightness()\n",
    "        self.rand_light_noise = RandomLightingNoise()\n",
    "\n",
    "    def __call__(self, image, boxes, labels):\n",
    "        im = image.copy()\n",
    "        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n",
    "        if random.randint(2):\n",
    "            distort = Compose(self.pd[:-1])\n",
    "        else:\n",
    "            distort = Compose(self.pd[1:])\n",
    "        im, boxes, labels = distort(im, boxes, labels)\n",
    "        return self.rand_light_noise(im, boxes, labels)\n",
    "\n",
    "\n",
    "class SSDAugmentation(object):\n",
    "    def __init__(self, size=300, mean=(104, 117, 123)):\n",
    "        self.mean = mean\n",
    "        self.size = size\n",
    "        self.augment = Compose([\n",
    "            ConvertFromInts(),\n",
    "            ToAbsoluteCoords(),\n",
    "            PhotometricDistort(),\n",
    "            Expand(self.mean),\n",
    "            RandomSampleCrop(),\n",
    "            RandomMirror(),\n",
    "            ToPercentCoords(),\n",
    "            Resize(self.size),\n",
    "            SubtractMeans(self.mean)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img, boxes, labels):\n",
    "        return self.augment(img, boxes, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rb_rrPgLTeM-"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SSD(nn.Module):\n",
    "    \"\"\"Single Shot Multibox Architecture\n",
    "    The network is composed of a base VGG network followed by the\n",
    "    added multibox conv layers.  Each multibox layer branches into\n",
    "        1) conv2d for class conf scores\n",
    "        2) conv2d for localization predictions\n",
    "        3) associated priorbox layer to produce default bounding\n",
    "           boxes specific to the layer's feature map size.\n",
    "    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "\n",
    "    Args:\n",
    "        phase: (string) Can be \"test\" or \"train\"\n",
    "        size: input image size\n",
    "        base: VGG16 layers for input, size of either 300 or 500\n",
    "        extras: extra layers that feed to multibox loc and conf layers\n",
    "        head: \"multibox head\" consists of loc and conf conv layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, phase, size, base, extras, head, num_classes):\n",
    "        super(SSD, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.num_classes = num_classes\n",
    "        self.cfg = _sixray\n",
    "        self.priorbox = PriorBox(self.cfg)\n",
    "        with torch.no_grad():\n",
    "          self.priors = Variable(self.priorbox.forward())\n",
    "        self.size = size\n",
    "\n",
    "        # SSD network\n",
    "        self.vgg = nn.ModuleList(base)\n",
    "        # Layer learns to scale the l2 normalized features from conv4_3\n",
    "        self.L2Norm = L2Norm(512, 20)\n",
    "        self.extras = nn.ModuleList(extras)\n",
    "\n",
    "        self.loc = nn.ModuleList(head[0])\n",
    "        self.conf = nn.ModuleList(head[1])\n",
    "\n",
    "        if phase == 'test':\n",
    "            self.softmax = nn.Softmax(dim=-1)\n",
    "            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies network layers and ops on input image(s) x.\n",
    "\n",
    "        Args:\n",
    "            x: input image or batch of images. Shape: [batch,3,300,300].\n",
    "\n",
    "        Return:\n",
    "            Depending on phase:\n",
    "            test:\n",
    "                Variable(tensor) of output class label predictions,\n",
    "                confidence score, and corresponding location predictions for\n",
    "                each object detected. Shape: [batch,topk,7]\n",
    "\n",
    "            train:\n",
    "                list of concat outputs from:\n",
    "                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n",
    "                    2: localization layers, Shape: [batch,num_priors*4]\n",
    "                    3: priorbox layers, Shape: [2,num_priors*4]\n",
    "        \"\"\"\n",
    "        sources = list()\n",
    "        loc = list()\n",
    "        conf = list()\n",
    "\n",
    "        # apply vgg up to conv4_3 relu\n",
    "        for k in range(23):\n",
    "            x = self.vgg[k](x)\n",
    "\n",
    "        s = self.L2Norm(x)\n",
    "        sources.append(s)\n",
    "\n",
    "        # apply vgg up to fc7\n",
    "        for k in range(23, len(self.vgg)):\n",
    "            x = self.vgg[k](x)\n",
    "        sources.append(x)\n",
    "\n",
    "        # apply extra layers and cache source layer outputs\n",
    "        for k, v in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace=True)\n",
    "            if k % 2 == 1:\n",
    "                sources.append(x)\n",
    "\n",
    "        # apply multibox head to source layers\n",
    "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
    "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
    "\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "        if self.phase == \"test\":\n",
    "            output = self.detect(\n",
    "                loc.view(loc.size(0), -1, 4),                   # loc preds\n",
    "                self.softmax(conf.view(conf.size(0), -1,\n",
    "                             self.num_classes)),                # conf preds\n",
    "                self.priors.type(type(x.data))                  # default boxes\n",
    "            )\n",
    "        else:\n",
    "            output = (\n",
    "                loc.view(loc.size(0), -1, 4),\n",
    "                conf.view(conf.size(0), -1, self.num_classes),\n",
    "                self.priors\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    def load_weights(self, base_file):\n",
    "        other, ext = os.path.splitext(base_file)\n",
    "        if ext == '.pkl' or '.pth':\n",
    "            print('Loading weights into state dict...')\n",
    "            self.load_state_dict(torch.load(base_file,\n",
    "                                 map_location=lambda storage, loc: storage))\n",
    "            print('Finished!')\n",
    "        else:\n",
    "            print('Sorry only .pth and .pkl files supported.')\n",
    "\n",
    "\n",
    "# This function is derived from torchvision VGG make_layers()\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "def vgg(cfg, i, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        elif v == 'C':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6,\n",
    "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
    "    return layers\n",
    "\n",
    "\n",
    "def add_extras(cfg, i, batch_norm=False):\n",
    "    # Extra layers added to VGG for feature scaling\n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    flag = False\n",
    "    for k, v in enumerate(cfg):\n",
    "        if in_channels != 'S':\n",
    "            if v == 'S':\n",
    "                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n",
    "                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n",
    "            flag = not flag\n",
    "        in_channels = v\n",
    "    return layers\n",
    "\n",
    "\n",
    "def multibox(vgg, extra_layers, cfg, num_classes):\n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "    vgg_source = [21, -2]\n",
    "    for k, v in enumerate(vgg_source):\n",
    "        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n",
    "                                 cfg[k] * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n",
    "                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n",
    "    for k, v in enumerate(extra_layers[1::2], 2):\n",
    "        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n",
    "                                 * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n",
    "                                  * num_classes, kernel_size=3, padding=1)]\n",
    "    return vgg, extra_layers, (loc_layers, conf_layers)\n",
    "\n",
    "\n",
    "base = {\n",
    "    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
    "            512, 512, 512],\n",
    "    '512': [],\n",
    "}\n",
    "extras = {\n",
    "    '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],\n",
    "    '512': [],\n",
    "}\n",
    "mbox = {\n",
    "    '300': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n",
    "    '512': [],\n",
    "}\n",
    "\n",
    "\n",
    "def build_ssd(phase, size=300, num_classes=21):\n",
    "    if phase != \"test\" and phase != \"train\":\n",
    "        print(\"ERROR: Phase: \" + phase + \" not recognized\")\n",
    "        return\n",
    "    if size != 300:\n",
    "        print(\"ERROR: You specified size \" + repr(size) + \". However, \" +\n",
    "              \"currently only SSD300 (size=300) is supported!\")\n",
    "        return\n",
    "    base_, extras_, head_ = multibox(vgg(base[str(size)], 3),\n",
    "                                     add_extras(extras[str(size)], 1024),\n",
    "                                     mbox[str(size)], num_classes)\n",
    "    return SSD(phase, size, base_, extras_, head_, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "UfFe_LLUTxjX",
    "outputId": "afbf7566-d205-4b99-c762-1adb9addca35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coreless_battery00004025.txt', 'coreless_battery00004041.txt', 'coreless_battery00004057.txt', 'coreless_battery00004083.txt', 'coreless_battery00004106.txt', 'coreless_battery00004114.txt', 'coreless_battery00004133.txt', 'coreless_battery00004172.txt', 'coreless_battery00004179.txt', 'coreless_battery00004188.txt', 'coreless_battery00004234.txt', 'coreless_battery00004242.txt', 'coreless_battery00004255.txt', 'coreless_battery00004289.txt', 'coreless_battery00004127.txt', 'coreless_battery00004307.txt', 'coreless_battery00004327.txt', 'coreless_battery00004336.txt', 'coreless_battery00004366.txt', 'coreless_battery00004383.txt', 'coreless_battery00004411.txt', 'coreless_battery00004428.txt', 'coreless_battery00004461.txt', 'coreless_battery00004466.txt', 'coreless_battery00004514.txt', 'coreless_battery00004528.txt', 'coreless_battery00004534.txt', 'coreless_battery00004566.txt', 'coreless_battery00004600.txt', 'coreless_battery00004609.txt', 'coreless_battery00004621.txt', 'coreless_battery00004636.txt', 'coreless_battery00004667.txt', 'coreless_battery00004377.txt', 'coreless_battery00004551.txt', 'coreless_battery00004734.txt', 'coreless_battery00004738.txt', 'coreless_battery00004740.txt', 'coreless_battery00004766.txt', 'coreless_battery00004788.txt', 'coreless_battery00004820.txt', 'coreless_battery00004843.txt', 'coreless_battery00004880.txt', 'coreless_battery00004903.txt', 'coreless_battery00004938.txt', 'coreless_battery00004956.txt', 'coreless_battery00004982.txt', 'coreless_battery00005000.txt', 'coreless_battery00005040.txt', 'coreless_battery00005061.txt', 'coreless_battery00005083.txt', 'coreless_battery00005104.txt', 'coreless_battery00004793.txt', 'coreless_battery00004942.txt', 'coreless_battery00005135.txt', 'coreless_battery00005157.txt', 'coreless_battery00005200.txt', 'coreless_battery00005210.txt', 'coreless_battery00005221.txt', 'coreless_battery00005252.txt', 'coreless_battery00005260.txt', 'coreless_battery00005283.txt', 'coreless_battery00005306.txt', 'coreless_battery00005328.txt', 'coreless_battery00005346.txt', 'coreless_battery00005355.txt', 'coreless_battery00005406.txt', 'coreless_battery00005411.txt', 'coreless_battery00005426.txt', 'coreless_battery00005431.txt', 'coreless_battery00005460.txt', 'coreless_battery00005521.txt', 'coreless_battery00005189.txt', 'coreless_battery00005483.txt', 'coreless_battery00005532.txt', 'coreless_battery00005556.txt', 'coreless_battery00005577.txt', 'coreless_battery00005595.txt', 'coreless_battery00005614.txt', 'coreless_battery00005638.txt', 'coreless_battery00005664.txt', 'coreless_battery00005671.txt', 'coreless_battery00005696.txt', 'coreless_battery00005726.txt', 'coreless_battery00005734.txt', 'coreless_battery00005766.txt', 'coreless_battery00005793.txt', 'coreless_battery00005825.txt', 'coreless_battery00005832.txt', 'coreless_battery00005843.txt', 'coreless_battery00005884.txt', 'coreless_battery00005888.txt', 'coreless_battery00005646.txt', 'coreless_battery00005920.txt', 'coreless_battery00005901.txt', 'coreless_battery00005914.txt', 'coreless_battery00005935.txt', 'coreless_battery00005955.txt', 'coreless_battery00005969.txt', 'coreless_battery00005986.txt', 'coreless_battery00002052.txt', 'coreless_battery00002092.txt', 'coreless_battery00002103.txt', 'coreless_battery00002109.txt', 'coreless_battery00002139.txt', 'coreless_battery00002179.txt', 'coreless_battery00002189.txt', 'coreless_battery00002209.txt', 'coreless_battery00002241.txt', 'coreless_battery00002290.txt', 'coreless_battery00002297.txt', 'coreless_battery00002331.txt', 'coreless_battery00002345.txt', 'coreless_battery00002355.txt', 'coreless_battery00002389.txt', 'coreless_battery00002381.txt', 'coreless_battery00002085.txt', 'coreless_battery00002416.txt', 'coreless_battery00002422.txt', 'coreless_battery00002430.txt', 'coreless_battery00002480.txt', 'coreless_battery00002488.txt', 'coreless_battery00002509.txt', 'coreless_battery00002527.txt', 'coreless_battery00002585.txt', 'coreless_battery00002592.txt', 'coreless_battery00002601.txt', 'coreless_battery00002631.txt', 'coreless_battery00002644.txt', 'coreless_battery00002661.txt', 'coreless_battery00002681.txt', 'coreless_battery00002701.txt', 'coreless_battery00002708.txt', 'coreless_battery00002748.txt', 'coreless_battery00002781.txt', 'coreless_battery00002790.txt', 'coreless_battery00002610.txt', 'coreless_battery00002806.txt', 'coreless_battery00002833.txt', 'coreless_battery00002849.txt', 'coreless_battery00002860.txt', 'coreless_battery00002886.txt', 'coreless_battery00002904.txt', 'coreless_battery00002928.txt', 'coreless_battery00002957.txt', 'coreless_battery00002967.txt', 'coreless_battery00002984.txt', 'coreless_battery00003010.txt', 'coreless_battery00003039.txt', 'coreless_battery00003046.txt', 'coreless_battery00003064.txt', 'coreless_battery00003106.txt', 'coreless_battery00003112.txt', 'coreless_battery00003129.txt', 'coreless_battery00003146.txt', 'coreless_battery00002873.txt', 'coreless_battery00002952.txt', 'coreless_battery00003180.txt', 'coreless_battery00003208.txt', 'coreless_battery00003233.txt', 'coreless_battery00003249.txt', 'coreless_battery00003295.txt', 'coreless_battery00003303.txt', 'coreless_battery00003316.txt', 'coreless_battery00003360.txt', 'coreless_battery00003371.txt', 'coreless_battery00003384.txt', 'coreless_battery00003406.txt', 'coreless_battery00003438.txt', 'coreless_battery00003465.txt', 'coreless_battery00003474.txt', 'coreless_battery00003497.txt', 'coreless_battery00003523.txt', 'coreless_battery00003530.txt', 'coreless_battery00003545.txt', 'coreless_battery00003338.txt', 'coreless_battery00003579.txt', 'coreless_battery00003588.txt', 'coreless_battery00003594.txt', 'coreless_battery00003644.txt', 'coreless_battery00003654.txt', 'coreless_battery00003660.txt', 'coreless_battery00003699.txt', 'coreless_battery00003725.txt', 'coreless_battery00003737.txt', 'coreless_battery00003743.txt', 'coreless_battery00003767.txt', 'coreless_battery00003816.txt', 'coreless_battery00003822.txt', 'coreless_battery00003829.txt', 'coreless_battery00003860.txt', 'coreless_battery00003884.txt', 'coreless_battery00003917.txt', 'coreless_battery00003927.txt', 'coreless_battery00003642.txt', 'coreless_battery00003903.txt', 'coreless_battery00003956.txt', 'coreless_battery00003973.txt', 'coreless_battery00003992.txt', 'coreless_battery00004015.txt', 'core_battery00000004.txt', 'core_battery00000021.txt', 'coreless_battery00000071.txt', 'coreless_battery00000081.txt', 'coreless_battery00000089.txt', 'coreless_battery00000119.txt', 'coreless_battery00000167.txt', 'coreless_battery00000175.txt', 'coreless_battery00000182.txt', 'coreless_battery00000216.txt', 'coreless_battery00000237.txt', 'coreless_battery00000264.txt', 'coreless_battery00000279.txt', 'coreless_battery00000288.txt', 'coreless_battery00000334.txt', 'coreless_battery00000341.txt', 'coreless_battery00000359.txt', 'coreless_battery00000375.txt', 'coreless_battery00000403.txt', 'coreless_battery00000423.txt', 'core_battery00000135.txt', 'core_battery00000384.txt', 'coreless_battery00000474.txt', 'coreless_battery00000479.txt', 'coreless_battery00000485.txt', 'coreless_battery00000530.txt', 'coreless_battery00000551.txt', 'coreless_battery00000565.txt', 'coreless_battery00000592.txt', 'coreless_battery00000608.txt', 'coreless_battery00000615.txt', 'coreless_battery00000655.txt', 'coreless_battery00000678.txt', 'coreless_battery00000726.txt', 'coreless_battery00000735.txt', 'coreless_battery00000761.txt', 'coreless_battery00000801.txt', 'coreless_battery00000811.txt', 'coreless_battery00000818.txt', 'coreless_battery00000844.txt', 'coreless_battery00000851.txt', 'coreless_battery00000877.txt', 'coreless_battery00000895.txt', 'coreless_battery00000921.txt', 'coreless_battery00000938.txt', 'coreless_battery00000974.txt', 'coreless_battery00000989.txt', 'coreless_battery00000999.txt', 'coreless_battery00001053.txt', 'coreless_battery00001059.txt', 'coreless_battery00001067.txt', 'coreless_battery00001090.txt', 'coreless_battery00001108.txt', 'coreless_battery00001134.txt', 'coreless_battery00001167.txt', 'coreless_battery00001174.txt', 'coreless_battery00001181.txt', 'coreless_battery00001213.txt', 'coreless_battery00001064.txt', 'coreless_battery00001114.txt', 'coreless_battery00001245.txt', 'coreless_battery00001301.txt', 'coreless_battery00001310.txt', 'coreless_battery00001323.txt', 'coreless_battery00001343.txt', 'coreless_battery00001361.txt', 'coreless_battery00001389.txt', 'coreless_battery00001406.txt', 'coreless_battery00001429.txt', 'coreless_battery00001455.txt', 'coreless_battery00001480.txt', 'coreless_battery00001494.txt', 'coreless_battery00001532.txt', 'coreless_battery00001541.txt', 'coreless_battery00001559.txt', 'coreless_battery00001576.txt', 'coreless_battery00001606.txt', 'coreless_battery00001487.txt', 'coreless_battery00001591.txt', 'coreless_battery00001636.txt', 'coreless_battery00001664.txt', 'coreless_battery00001695.txt', 'coreless_battery00001707.txt', 'coreless_battery00001718.txt', 'coreless_battery00001750.txt', 'coreless_battery00001780.txt', 'coreless_battery00001787.txt', 'coreless_battery00001801.txt', 'coreless_battery00001849.txt', 'coreless_battery00001859.txt', 'coreless_battery00001890.txt', 'coreless_battery00001912.txt', 'coreless_battery00001922.txt', 'coreless_battery00001925.txt', 'coreless_battery00001983.txt', 'coreless_battery00001993.txt', 'coreless_battery00002012.txt', 'coreless_battery00001992.txt', 'coreless_battery00002031.txt', 'coreless_battery00002038.txt', 'core_battery00004092.txt', 'core_battery00004102.txt', 'core_battery00004143.txt', 'core_battery00004162.txt', 'core_battery00004192.txt', 'core_battery00004201.txt', 'core_battery00004220.txt', 'core_battery00004253.txt', 'core_battery00004265.txt', 'core_battery00004288.txt', 'core_battery00004295.txt', 'core_battery00004328.txt', 'core_battery00004362.txt', 'core_battery00003926.txt', 'core_battery00004375.txt', 'core_battery00004396.txt', 'core_battery00004414.txt', 'core_battery00004445.txt', 'core_battery00004465.txt', 'core_battery00004481.txt', 'core_battery00004502.txt', 'core_battery00004527.txt', 'core_battery00004524.txt', 'core_battery00004559.txt', 'core_battery00004598.txt', 'core_battery00004608.txt', 'core_battery00004644.txt', 'core_battery00004654.txt', 'core_battery00004688.txt', 'core_battery00004693.txt', 'core_battery00004721.txt', 'core_battery00004745.txt', 'core_battery00004756.txt', 'core_battery00004782.txt', 'core_battery00004540.txt', 'core_battery00004816.txt', 'core_battery00004807.txt', 'core_battery00004845.txt', 'core_battery00004851.txt', 'core_battery00004864.txt', 'core_battery00004866.txt', 'core_battery00004908.txt', 'core_battery00004919.txt', 'core_battery00004941.txt', 'core_battery00004957.txt', 'core_battery00004972.txt', 'core_battery00004994.txt', 'core_battery00005013.txt', 'core_battery00005037.txt', 'core_battery00005048.txt', 'core_battery00005076.txt', 'core_battery00005105.txt', 'core_battery00005112.txt', 'core_battery00005123.txt', 'core_battery00005150.txt', 'core_battery00004964.txt', 'core_battery00005170.txt', 'core_battery00005196.txt', 'core_battery00005229.txt', 'core_battery00005241.txt', 'core_battery00005267.txt', 'core_battery00005281.txt', 'core_battery00005309.txt', 'core_battery00005329.txt', 'core_battery00005367.txt', 'core_battery00005374.txt', 'core_battery00005390.txt', 'core_battery00005427.txt', 'core_battery00005444.txt', 'core_battery00005468.txt', 'core_battery00005479.txt', 'core_battery00005512.txt', 'core_battery00005516.txt', 'core_battery00005540.txt', 'core_battery00005586.txt', 'core_battery00005590.txt', 'core_battery00005220.txt', 'core_battery00005620.txt', 'core_battery00005632.txt', 'core_battery00005653.txt', 'core_battery00005670.txt', 'core_battery00005708.txt', 'core_battery00005721.txt', 'core_battery00005732.txt', 'core_battery00005758.txt', 'core_battery00005790.txt', 'core_battery00005806.txt', 'core_battery00005837.txt', 'core_battery00005860.txt', 'core_battery00005864.txt', 'core_battery00005921.txt', 'core_battery00005929.txt', 'core_battery00005944.txt', 'core_battery00005968.txt', 'core_battery00006004.txt', 'core_battery00006027.txt', 'core_battery00005592.txt', 'core_battery00005813.txt', 'core_battery00006041.txt', 'core_battery00006068.txt', 'core_battery00001968.txt', 'core_battery00002015.txt', 'core_battery00002022.txt', 'core_battery00002054.txt', 'core_battery00002080.txt', 'core_battery00002094.txt', 'core_battery00002096.txt', 'core_battery00002131.txt', 'core_battery00001921.txt', 'core_battery00002149.txt', 'core_battery00002175.txt', 'core_battery00002187.txt', 'core_battery00002223.txt', 'core_battery00002228.txt', 'core_battery00002235.txt', 'core_battery00002259.txt', 'core_battery00002281.txt', 'core_battery00002294.txt', 'core_battery00002315.txt', 'core_battery00002350.txt', 'core_battery00002391.txt', 'core_battery00002397.txt', 'core_battery00002414.txt', 'core_battery00002448.txt', 'core_battery00002476.txt', 'core_battery00002500.txt', 'core_battery00002533.txt', 'core_battery00002517.txt', 'core_battery00002553.txt', 'core_battery00002444.txt', 'core_battery00002567.txt', 'core_battery00002573.txt', 'core_battery00002629.txt', 'core_battery00002637.txt', 'core_battery00002665.txt', 'core_battery00002692.txt', 'core_battery00002717.txt', 'core_battery00002726.txt', 'core_battery00002753.txt', 'core_battery00002797.txt', 'core_battery00002801.txt', 'core_battery00002819.txt', 'core_battery00002863.txt', 'core_battery00002867.txt', 'core_battery00002898.txt', 'core_battery00002913.txt', 'core_battery00002937.txt', 'core_battery00002963.txt', 'core_battery00002985.txt', 'core_battery00003000.txt', 'core_battery00002882.txt', 'core_battery00003045.txt', 'core_battery00003057.txt', 'core_battery00003068.txt', 'core_battery00003100.txt', 'core_battery00003135.txt', 'core_battery00003166.txt', 'core_battery00003185.txt', 'core_battery00003199.txt', 'core_battery00003217.txt', 'core_battery00003245.txt', 'core_battery00003253.txt', 'core_battery00003277.txt', 'core_battery00003310.txt', 'core_battery00003318.txt', 'core_battery00003330.txt', 'core_battery00003355.txt', 'core_battery00003379.txt', 'core_battery00003388.txt', 'core_battery00003419.txt', 'core_battery00003214.txt', 'core_battery00003329.txt', 'core_battery00003454.txt', 'core_battery00003478.txt', 'core_battery00003527.txt', 'core_battery00003535.txt', 'core_battery00003542.txt', 'core_battery00003580.txt', 'core_battery00003615.txt', 'core_battery00003622.txt', 'core_battery00003649.txt', 'core_battery00003684.txt', 'core_battery00003696.txt', 'core_battery00003700.txt', 'core_battery00003751.txt', 'core_battery00003762.txt', 'core_battery00003777.txt', 'core_battery00003798.txt', 'core_battery00003819.txt', 'core_battery00003850.txt', 'core_battery00003855.txt', 'core_battery00003638.txt', 'core_battery00003766.txt', 'core_battery00003912.txt', 'core_battery00003928.txt', 'core_battery00003947.txt', 'core_battery00003958.txt', 'core_battery00004012.txt', 'core_battery00004045.txt', 'core_battery00004067.txt', 'coreless_battery00000001.txt', 'coreless_battery00000023.txt', 'core_battery00000036.txt', 'core_battery00000046.txt', 'core_battery00000073.txt', 'coreless_battery00000075.txt', 'coreless_battery00000165.txt', 'coreless_battery00000353.txt', 'core_battery00000123.txt', 'core_battery00000158.txt', 'core_battery00000164.txt', 'core_battery00000186.txt', 'core_battery00000224.txt', 'core_battery00000235.txt', 'core_battery00000255.txt', 'core_battery00000281.txt', 'core_battery00000315.txt', 'core_battery00000324.txt', 'core_battery00000348.txt', 'core_battery00000367.txt', 'core_battery00000401.txt', 'core_battery00000416.txt', 'core_battery00000425.txt', 'core_battery00000455.txt', 'core_battery00000487.txt', 'core_battery00000504.txt', 'core_battery00000520.txt', 'core_battery00000554.txt', 'core_battery00000572.txt', 'core_battery00000590.txt', 'core_battery00000598.txt', 'core_battery00000638.txt', 'core_battery00000653.txt', 'core_battery00000665.txt', 'core_battery00000704.txt', 'core_battery00000718.txt', 'core_battery00000713.txt', 'core_battery00000745.txt', 'core_battery00000766.txt', 'core_battery00000779.txt', 'core_battery00000808.txt', 'core_battery00000827.txt', 'core_battery00000840.txt', 'core_battery00000571.txt', 'core_battery00000870.txt', 'core_battery00000880.txt', 'core_battery00000890.txt', 'core_battery00000923.txt', 'core_battery00000939.txt', 'core_battery00000944.txt', 'core_battery00000987.txt', 'core_battery00001004.txt', 'core_battery00001022.txt', 'core_battery00001042.txt', 'core_battery00001083.txt', 'core_battery00001095.txt', 'core_battery00001124.txt', 'core_battery00001141.txt', 'core_battery00001161.txt', 'core_battery00001180.txt', 'core_battery00001226.txt', 'core_battery00001233.txt', 'core_battery00001251.txt', 'core_battery00001260.txt', 'core_battery00001272.txt', 'core_battery00001277.txt', 'core_battery00001296.txt', 'core_battery00001309.txt', 'core_battery00001354.txt', 'core_battery00001368.txt', 'core_battery00001395.txt', 'core_battery00001424.txt', 'core_battery00001434.txt', 'core_battery00001444.txt', 'core_battery00001483.txt', 'core_battery00001490.txt', 'core_battery00001506.txt', 'core_battery00001552.txt', 'core_battery00001563.txt', 'core_battery00001583.txt', 'core_battery00001600.txt', 'core_battery00001631.txt', 'core_battery00001639.txt', 'core_battery00001648.txt', 'core_battery00001362.txt', 'core_battery00001505.txt', 'coreless_battery00002793.txt', 'core_battery00001698.txt', 'core_battery00001735.txt', 'core_battery00001741.txt', 'core_battery00001769.txt', 'core_battery00001791.txt', 'core_battery00001812.txt', 'core_battery00001830.txt', 'core_battery00001839.txt', 'core_battery00001870.txt', 'core_battery00001885.txt', 'core_battery00001907.txt', 'core_battery00001935.txt', 'core_battery00001942.txt']\n",
      "Initializing weights...\n",
      "Loading the dataset...\n",
      "Training SSD on: sixray\n",
      "Using the specified args:\n",
      "Namespace(basenet='VOC.pth', batch_size=30, cuda=True, dataset='VOC', dataset_root='/content/gdrive/My Drive/weights/', gamma=0.1, lr=0.0002, momentum=0.9, num_workers=4, resume=None, save_folder='weights/', start_iter=0, visdom=False, weight_decay=0.0005)\n",
      "iter 1000 || Loss: 3.7460 || loss_l: 1.294632 || loss_c: 2.451363 \n",
      "\n",
      "iter 2000 || Loss: 3.2901 || loss_l: 0.858658 || loss_c: 2.431415 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data as data\n",
    "import argparse\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Single Shot MultiBox Detector Training With Pytorch')\n",
    "train_set = parser.add_mutually_exclusive_group()\n",
    "parser.add_argument('--dataset', default='VOC', choices=['VOC', 'COCO'],\n",
    "                    type=str, help='VOC or COCO')\n",
    "parser.add_argument('--dataset_root', default='/content/gdrive/My Drive/weights/',\n",
    "                    help='Dataset root directory path')\n",
    "parser.add_argument('--basenet', default='VOC.pth',\n",
    "                    help='Pretrained base model')\n",
    "parser.add_argument('--batch_size', default=30, type=int,\n",
    "                    help='Batch size for training')\n",
    "parser.add_argument('--resume', default=None, type=str,\n",
    "                    help='Checkpoint state_dict file to resume training from')\n",
    "parser.add_argument('--start_iter', default=0, type=int,\n",
    "                    help='Resume training at this iter')\n",
    "parser.add_argument('--num_workers', default=4, type=int,\n",
    "                    help='Number of workers used in dataloading')\n",
    "parser.add_argument('--cuda', default=True, type=str2bool,\n",
    "                    help='Use CUDA to train model')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.0002, type=float,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                    help='Momentum value for optim')\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float,\n",
    "                    help='Weight decay for SGD')\n",
    "parser.add_argument('--gamma', default=0.1, type=float,\n",
    "                    help='Gamma update for SGD')\n",
    "parser.add_argument('--visdom', default=False, type=str2bool,\n",
    "                    help='Use visdom for loss visualization')\n",
    "parser.add_argument('--save_folder', default='weights/',\n",
    "                    help='Directory for saving checkpoint models')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if args.cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n",
    "              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "if not os.path.exists(args.save_folder):\n",
    "    os.mkdir(args.save_folder)\n",
    "\n",
    "# import visdom\n",
    "# viz = visdom.Visdom()\n",
    "# python -m visdom.server\n",
    "\n",
    "def train():\n",
    "    # if args.dataset == 'COCO':\n",
    "    #     if args.dataset_root == VOC_ROOT:\n",
    "    #         if not os.path.exists(COCO_ROOT):\n",
    "    #             parser.error('Must specify dataset_root if specifying dataset')\n",
    "    #         print(\"WARNING: Using default COCO dataset_root because \" +\n",
    "    #               \"--dataset_root was not specified.\")\n",
    "    #         args.dataset_root = COCO_ROOT\n",
    "    #     cfg = coco\n",
    "    #     dataset = COCODetection(root=args.dataset_root,\n",
    "    #                             transform=SSDAugmentation(cfg['min_dim'],\n",
    "    #                                                       MEANS))\n",
    "    # elif args.dataset == 'VOC':\n",
    "    #     if args.dataset_root == COCO_ROOT:\n",
    "    #         parser.error('Must specify dataset if specifying dataset_root')\n",
    "    #     cfg = voc\n",
    "    #     dataset = VOCDetection(root=args.dataset_root,\n",
    "    #                            transform=SSDAugmentation(cfg['min_dim'],\n",
    "    #                                                      MEANS))\n",
    "\n",
    "    cfg = _sixray\n",
    "    cfg['num_classes'] = 3\n",
    "\n",
    "    dataset_mean = (104, 117, 123)\n",
    "    dataset = SIXrayDetection(\"/content/gdrive/My Drive/all/\", \"\",\n",
    "                              # BaseTransform(cfg['min_dim'], MEANS),\n",
    "                              SSDAugmentation(cfg['min_dim'], MEANS),\n",
    "                              SIXrayAnnotationTransform())\n",
    "\n",
    "    # if args.visdom:\n",
    "\n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    net = ssd_net\n",
    "\n",
    "    if args.cuda:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    if args.resume:\n",
    "        print('Resuming training, loading {}...'.format(args.resume))\n",
    "        ssd_net.load_weights(args.resume)\n",
    "    else:\n",
    "        vgg_weights = torch.load(args.save_folder + args.basenet)\n",
    "        # from collections import OrderedDict\n",
    "        # new_state_dict = OrderedDict()\n",
    "        # for k, v in vgg_weights.items():\n",
    "        #     name = k[4:] # remove 'module.' of dataparallel\n",
    "        #     new_state_dict[name]=v\n",
    "        # print('Loading base network...')\n",
    "        ssd_net.load_state_dict(vgg_weights)\n",
    "        # ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "    \n",
    "    if args.cuda:\n",
    "        net = net.cuda()\n",
    "\n",
    "    if not args.resume:\n",
    "        print('Initializing weights...')\n",
    "        # initialize newly added layers' weights with xavier method\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "    criterion = MultiBoxLoss(cfg, 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, args.cuda)\n",
    "\n",
    "    net.train()\n",
    "    # loss counters\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // args.batch_size\n",
    "    print('Training SSD on:', dataset.name)\n",
    "    print('Using the specified args:')\n",
    "    print(args)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "    # if args.visdom:\n",
    "    #     vis_title = 'SSD.PyTorch on ' + dataset.name\n",
    "    #     vis_legend = ['Loc Loss', 'Conf Loss', 'Total Loss']\n",
    "    #     iter_plot = create_vis_plot('Iteration', 'Loss', vis_title, vis_legend)\n",
    "    #     epoch_plot = create_vis_plot('Epoch', 'Loss', vis_title, vis_legend)\n",
    "    i = 0\n",
    "    for iteration in range(args.start_iter,26):\n",
    "        data_loader = data.DataLoader(dataset, 20,\n",
    "                                      num_workers=args.num_workers,\n",
    "                                      shuffle=True, collate_fn=detection_collate,\n",
    "                                      pin_memory=True)\n",
    "        # create batch iterator\n",
    "        batch_iterator = iter(data_loader)\n",
    "        # if args.visdom and iteration != 0 and (iteration % epoch_size == 0):\n",
    "        #     update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None,\n",
    "        #                     'append', epoch_size)\n",
    "        #     # reset epoch loss counters\n",
    "        #     loc_loss = 0\n",
    "        #     conf_loss = 0\n",
    "        #     epoch += 1\n",
    "        # if i in cfg['lr_steps']:\n",
    "        # if iteration in [1, 2, 4]:\n",
    "        # if iteration == 19 or iteration == 20 or iteration == 30: \n",
    "        #   step_index += 1\n",
    "        #   adjust_learning_rate(optimizer, args.gamma, step_index)\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            adjust_learning_rate(optimizer, args.gamma, step_index)\n",
    "        for j in range(240):\n",
    "          i += 1\n",
    "          # load train data\n",
    "          images, targets = next(batch_iterator)\n",
    "\n",
    "          if args.cuda:\n",
    "              images = Variable(images.cuda())\n",
    "              with torch.no_grad():\n",
    "                targets = [Variable(ann.cuda()) for ann in targets]\n",
    "          else:\n",
    "            images = Variable(images)\n",
    "            with torch.no_grad():\n",
    "              targets = [Variable(ann) for ann in targets]\n",
    "          # forward\n",
    "          t0 = time.time()\n",
    "          out = net(images)\n",
    "          # backprop\n",
    "          optimizer.zero_grad()\n",
    "          # print(type(out))\n",
    "          # print(out)\n",
    "          # print(type(targets))\n",
    "          loss_l, loss_c = criterion(out, targets)\n",
    "          loss = loss_l + loss_c\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          t1 = time.time()\n",
    "          # https://github.com/NVIDIA/flownet2-pytorch/issues/113\n",
    "          loc_loss += loss_l.data\n",
    "          conf_loss += loss_c.data\n",
    "        # if iteration % 10 == 0:\n",
    "        #     print('timer: %.4f sec.' % (t1 - t0))\n",
    "        #     print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data[0]), end=' ')\n",
    "\n",
    "        # if args.visdom:\n",
    "        #     update_vis_plot(iteration, loss_l.data[0], loss_c.data[0],\n",
    "        #                     iter_plot, epoch_plot, 'append')\n",
    "\n",
    "        # if iteration != 0 and iteration % 5000 == 0:\n",
    "        #     print('Saving state, iter:', iteration)\n",
    "        #     torch.save(ssd_net.state_dict(), 'weights/ssd300_COCO_' +\n",
    "        #                repr(iteration) + '.pth')\n",
    "          if (i%1000 == 0):\n",
    "              print('iter ' + repr(i) + ' || Loss: %.4f || loss_l: %4f || loss_c: %4f \\n' % (loss.data, loss_l.data, loss_c.data))\n",
    "              torch.save(ssd_net.state_dict(),\n",
    "                        args.save_folder + '' + 'sixray_9_1_' + str(i) + '.pth')\n",
    "    # 3, 19200\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    lr = args.lr * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def xavier(param):\n",
    "    init.xavier_uniform_(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "# def create_vis_plot(_xlabel, _ylabel, _title, _legend):\n",
    "#     return viz.line(\n",
    "#         X=torch.zeros((1,)).cpu(),\n",
    "#         Y=torch.zeros((1, 3)).cpu(),\n",
    "#         opts=dict(\n",
    "#             xlabel=_xlabel,\n",
    "#             ylabel=_ylabel,\n",
    "#             title=_title,\n",
    "#             legend=_legend\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "\n",
    "# def update_vis_plot(iteration, loc, conf, window1, window2, update_type,\n",
    "#                     epoch_size=1):\n",
    "#     viz.line(\n",
    "#         X=torch.ones((1, 3)).cpu() * iteration,\n",
    "#         Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu() / epoch_size,\n",
    "#         win=window1,\n",
    "#         update=update_type\n",
    "#     )\n",
    "#     # initialize epoch plot on first iteration\n",
    "#     if iteration == 0:\n",
    "#         viz.line(\n",
    "#             X=torch.zeros((1, 3)).cpu(),\n",
    "#             Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu(),\n",
    "#             win=window2,\n",
    "#             update=True\n",
    "#         )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLU2AZFxr6Je"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9x_-9jFaNZ4d"
   },
   "outputs": [],
   "source": [
    "print(1001 % 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8GeY1iUXxri"
   },
   "outputs": [],
   "source": [
    "# \"\"\"Adapted from:\n",
    "#     @longcw faster_rcnn_pytorch: https://github.com/longcw/faster_rcnn_pytorch\n",
    "#     @rbgirshick py-faster-rcnn https://github.com/rbgirshick/py-faster-rcnn\n",
    "#     Licensed under The MIT License [see LICENSE for details]\n",
    "# \"\"\"\n",
    "\n",
    "# from __future__ import print_function\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.backends.cudnn as cudnn\n",
    "# from torch.autograd import Variable\n",
    "# import torch.utils.data as data\n",
    "# import sys\n",
    "# import os\n",
    "# import os.path as osp\n",
    "# import time\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import cv2\n",
    "# import shutil\n",
    "\n",
    "# if sys.version_info[0] == 2:\n",
    "#     import xml.etree.cElementTree as ET\n",
    "# else:\n",
    "#     import xml.etree.ElementTree as ET\n",
    "\n",
    "# labelmap = SIXray_CLASSES\n",
    "\n",
    "\n",
    "# def str2bool(v):\n",
    "#     return v.lower() in (\"yes\", \"true\", \"t\", \"a1\")\n",
    "\n",
    "\n",
    "# EPOCH = 5\n",
    "# GPUID = '3'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPUID\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description='Single Shot MultiBox Detector Evaluation')\n",
    "# parser.add_argument('--trained_model',\n",
    "#                     default=\"\", type=str,\n",
    "#                     help='Trained state_dict file path to open')\n",
    "# parser.add_argument(  # '--save_folder', default='/media/dsg3/husheng/eval/', type=str,\n",
    "#     '--save_folder',\n",
    "#     default=\"\", type=str,\n",
    "#     help='File path to save results')\n",
    "# parser.add_argument('--confidence_threshold', default=0.2, type=float,\n",
    "#                     help='Detection confidence threshold')\n",
    "# parser.add_argument('--top_k', default=5, type=int,\n",
    "#                     help='Further restrict the number of predictions to parse')\n",
    "# parser.add_argument('--cuda', default=True, type=str2bool,\n",
    "#                     help='Use cuda to train model')\n",
    "# parser.add_argument('--SIXray_root', default=SIXray_ROOT,\n",
    "#                     help='Location of VOC root directory')\n",
    "# parser.add_argument('--cleanup', default=True, type=str2bool,\n",
    "#                     help='Cleanup and remove results files following eval')\n",
    "# parser.add_argument('--imagesetfile',\n",
    "#                     # default='/media/dsg3/datasets/SIXray/dataset-test.txt', type=str,\n",
    "#                     default=\"/media/trs2/Xray20190723/train_test_txt/battery_sub/sub_test_core_coreless.txt\", type=str,\n",
    "#                     help='imageset file path to open')\n",
    "\n",
    "# args = parser.parse_args([])\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     if args.cuda:\n",
    "#         torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#     if not args.cuda:\n",
    "#         # print(\"WARNING: It looks like you have a CUDA device, but aren't using \\\n",
    "#         #         CUDA.  Run with --cuda for optimal eval speed.\")\n",
    "#         torch.set_default_tensor_type('torch.FloatTensor')\n",
    "# else:\n",
    "#     torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "# annopath = os.path.join(args.SIXray_root, 'Annotation', '%s.txt')\n",
    "# imgpath = os.path.join(args.SIXray_root, 'Image', '%s.jpg')\n",
    "\n",
    "# # imgsetpath = os.path.join(args.voc_root, 'VOC2007', 'ImageSets', 'Main', '{:s}.txt')\n",
    "\n",
    "# YEAR = '2007'\n",
    "\n",
    "# devkit_path = args.save_folder\n",
    "# dataset_mean = (104, 117, 123)\n",
    "# set_type = 'test'\n",
    "\n",
    "\n",
    "# class Timer(object):\n",
    "#     \"\"\"A simple timer.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.total_time = 0.\n",
    "#         self.calls = 0\n",
    "#         self.start_time = 0.\n",
    "#         self.diff = 0.\n",
    "#         self.average_time = 0.\n",
    "\n",
    "#     def tic(self):\n",
    "#         # using time.time instead of time.clock because time time.clock\n",
    "#         # does not normalize for multithreading\n",
    "#         self.start_time = time.time()\n",
    "\n",
    "#     def toc(self, average=True):\n",
    "#         self.diff = time.time() - self.start_time\n",
    "#         self.total_time += self.diff\n",
    "#         self.calls += 1\n",
    "#         self.average_time = self.total_time / self.calls\n",
    "#         if average:\n",
    "#             return self.average_time\n",
    "#         else:\n",
    "#             return self.diff\n",
    "\n",
    "\n",
    "# def parse_rec(filename,imgpath):\n",
    "#     \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "#     # tree = ET.parse(filename)\n",
    "#     # filename = filename[:-3] + 'txt'\n",
    "\n",
    "#     filename = filename.replace('.xml', '.txt')\n",
    "    \n",
    "#     #imagename0 = filename.replace('Anno_core_coreless_battery_sub_2000_500', 'cut_Image_core_coreless_battery_sub_2000_500')\n",
    "#     img_fold_name = imgpath.split('/')[-2]\n",
    "#     imagename0 = filename.replace('Annotation', img_fold_name)\n",
    "#     imagename1 = imagename0.replace('.txt', '.jpg')  # jpg form\n",
    "#     imagename2 = imagename0.replace('.txt', '.jpg')\n",
    "#     objects = []\n",
    "#     # 还需要同时打开图像，读入图像大小\n",
    "#     # print(imagename1)\n",
    "#     img = cv2.imread(imagename1)\n",
    "#     if img is None:\n",
    "#         img = cv2.imread(imagename2)\n",
    "#     height, width, channels = img.shape\n",
    "#     with open(filename, \"r\", encoding='utf-8') as f1:\n",
    "#         dataread = f1.readlines()\n",
    "#         for annotation in dataread:\n",
    "#             obj_struct = {}\n",
    "#             temp = annotation.split()\n",
    "#             name = temp[1]\n",
    "#             if name != '带电芯充电宝' and name != '不带电芯充电宝':\n",
    "#                 continue\n",
    "#             xmin = int(temp[2])\n",
    "#             # 只读取V视角的\n",
    "#             if int(xmin) > width:\n",
    "#                 continue\n",
    "#             if xmin < 0:\n",
    "#                 xmin = 1\n",
    "#             ymin = int(temp[3])\n",
    "#             if ymin < 0:\n",
    "#                 ymin = 1\n",
    "#             xmax = int(temp[4])\n",
    "#             if xmax > width: \n",
    "#                 xmax = width - 1\n",
    "#             ymax = int(temp[5])\n",
    "#             if ymax > height:\n",
    "#                 ymax = height - 1\n",
    "#             ##name\n",
    "#             obj_struct['name'] = name\n",
    "#             obj_struct['pose'] = 'Unspecified'\n",
    "#             obj_struct['truncated'] = 0\n",
    "#             obj_struct['difficult'] = 0\n",
    "#             obj_struct['bbox'] = [float(xmin) - 1,\n",
    "#                                   float(ymin) - 1,\n",
    "#                                   float(xmax) - 1,\n",
    "#                                   float(ymax) - 1]\n",
    "#             objects.append(obj_struct)\n",
    "\n",
    "#     '''\n",
    "#     for obj in tree.findall('object'):\n",
    "#         obj_struct = {}\n",
    "#         obj_struct['name'] = obj.find('name').text.lower().strip()\n",
    "#         obj_struct['pose'] = obj.find('pose').text\n",
    "#         obj_struct['truncated'] = int(obj.find('truncated').text)\n",
    "#         obj_struct['difficult'] = int(obj.find('difficult').text)\n",
    "#         bbox = obj.find('bndbox')\n",
    "#         obj_struct['bbox'] = [float(bbox.find('xmin').text) - 1,\n",
    "#                               float(bbox.find('ymin').text) - 1,\n",
    "#                               float(bbox.find('xmax').text) - 1,\n",
    "#                               float(bbox.find('ymax').text) - 1]\n",
    "#         objects.append(obj_struct)\n",
    "#     '''\n",
    "#     return objects\n",
    "\n",
    "\n",
    "# def get_output_dir(name, phase):\n",
    "#     \"\"\"Return the directory where experimental artifacts are placed.\n",
    "#     If the directory does not exist, it is created.\n",
    "#     A canonical path is built using the name from an imdb and a network\n",
    "#     (if not None).\n",
    "#     \"\"\"\n",
    "#     filedir = os.path.join(name, phase)\n",
    "#     if not os.path.exists(filedir):\n",
    "#         os.makedirs(filedir)\n",
    "#     return filedir\n",
    "\n",
    "\n",
    "# def get_voc_results_file_template(image_set, cls):\n",
    "#     # VOCdevkit/VOC2007/results/det_test_aeroplane.txt\n",
    "#     filename = 'det_' + image_set + '_%s.txt' % (cls)\n",
    "#     filedir = os.path.join(devkit_path, 'results')\n",
    "#     if not os.path.exists(filedir):\n",
    "#         os.makedirs(filedir)\n",
    "#     path = os.path.join(filedir, filename)\n",
    "#     return path\n",
    "\n",
    "\n",
    "# def write_voc_results_file(all_boxes, dataset):\n",
    "#     for cls_ind, cls in enumerate(labelmap):\n",
    "#         # print('Writing {:s} VOC results file'.format(cls))\n",
    "#         filename = get_voc_results_file_template(set_type, cls)\n",
    "#         with open(filename, 'wt') as f:\n",
    "#             for im_ind, index in enumerate(dataset.ids):\n",
    "#                 dets = all_boxes[cls_ind + 1][im_ind]\n",
    "#                 if dets == []:\n",
    "#                     continue\n",
    "#                 # the VOCdevkit expects a1-based indices\n",
    "#                 for k in range(dets.shape[0]):\n",
    "#                     f.write('{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n'.\n",
    "#                             format(index, dets[k, -1],\n",
    "#                                    dets[k, 0] + 1, dets[k, 1] + 1,\n",
    "#                                    dets[k, 2] + 1, dets[k, 3] + 1))\n",
    "\n",
    "\n",
    "# def do_python_eval(output_dir='output', use_07=False):\n",
    "#     cachedir = os.path.join(devkit_path, 'annotations_cache')\n",
    "#     aps = []\n",
    "#     # The PASCAL VOC metric changed in 2010\n",
    "#     use_07_metric = use_07\n",
    "#     # print('VOC07 metric? ' + ('Yes' if use_07_metric else 'No'))\n",
    "#     if not os.path.isdir(output_dir):\n",
    "#         os.mkdir(output_dir)\n",
    "#     for i, cls in enumerate(labelmap):\n",
    "#         filename = get_voc_results_file_template(set_type, cls)\n",
    "#         rec, prec, ap = voc_eval(\n",
    "#             filename, annopath,imgpath, args.imagesetfile, cls, cachedir,\n",
    "#             ovthresh=0.5, use_07_metric=use_07_metric)\n",
    "#         aps += [ap]\n",
    "#         # print('AP for {} = {:.4f}'.format(cls, ap))\n",
    "#         with open(os.path.join(output_dir, cls + '_pr.pkl'), 'wb') as f:\n",
    "#             pickle.dump({'rec': rec, 'prec': prec, 'ap': ap}, f)\n",
    "#     print(\"EPOCH, {:d}, mAP, {:.4f}, core_AP, {:.4f}, coreless_AP, {:.4f}\".format(EPOCH, np.mean(aps), aps[0], aps[1]))\n",
    "#     # print('Mean AP = {:.4f}'.format(np.mean(aps)))\n",
    "#     '''\n",
    "#     print('~~~~~~~~')\n",
    "#     print('Results:')\n",
    "#     for ap in aps:\n",
    "#         print('{:.3f}'.format(ap))\n",
    "#     print('{:.3f}'.format(np.mean(aps)))\n",
    "#     print('~~~~~~~~')\n",
    "#     print('--------------------------------------------------------------')\n",
    "#     print('Results computed with the **unofficial** Python eval code.')\n",
    "#     print('Results should be very close to the official MATLAB eval code.')\n",
    "#     print('--------------------------------------------------------------')\n",
    "#     '''\n",
    "\n",
    "\n",
    "# def voc_ap(rec, prec, use_07_metric=True):\n",
    "#     \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n",
    "#     Compute VOC AP given precision and recall.\n",
    "#     If use_07_metric is true, uses the\n",
    "#     VOC 07 11 point method (default:True).\n",
    "#     \"\"\"\n",
    "#     if use_07_metric:\n",
    "#         # 11 point metric\n",
    "#         ap = 0.\n",
    "#         for t in np.arange(0., 1.1, 0.1):\n",
    "#             if np.sum(rec >= t) == 0:\n",
    "#                 p = 0\n",
    "#             else:\n",
    "#                 p = np.max(prec[rec >= t])\n",
    "#             ap = ap + p / 11.\n",
    "#     else:\n",
    "#         # correct AP calculation\n",
    "#         # first append sentinel values at the end\n",
    "#         mrec = np.concatenate(([0.], rec, [1.]))\n",
    "#         mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "#         # compute the precision envelope\n",
    "#         for i in range(mpre.size - 1, 0, -1):\n",
    "#             mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "#         # to calculate area under PR curve, look for points\n",
    "#         # where X axis (recall) changes value\n",
    "#         i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "#         # and sum (\\Delta recall) * prec\n",
    "#         ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "#     return ap\n",
    "\n",
    "\n",
    "# def voc_eval(detpath,\n",
    "#              annopath,\n",
    "# \t\t\t       imgpath,\n",
    "#              imagesetfile,\n",
    "#              classname,\n",
    "#              cachedir,\n",
    "#              ovthresh=0.5,\n",
    "#              use_07_metric=True):\n",
    "#     \"\"\"rec, prec, ap = voc_eval(detpath,\n",
    "#                            annopath,\n",
    "#                            imagesetfile,\n",
    "#                            classname,\n",
    "#                            [ovthresh],\n",
    "#                            [use_07_metric])\n",
    "#     Top level function that does the PASCAL VOC evaluation.\n",
    "#     detpath: Path to detections\n",
    "#        detpath.format(classname) should produce the detection results file.\n",
    "#     annopath: Path to annotations\n",
    "#        annopath.format(imagename) should be the xml annotations file.\n",
    "#     imagesetfile: Text file containing the list of images, one image per line.\n",
    "#     classname: Category name (duh)\n",
    "#     cachedir: Directory for caching the annotations\n",
    "#     [ovthresh]: Overlap threshold (default = 0.5)\n",
    "#     [use_07_metric]: Whether to use VOC07's 11 point AP computation\n",
    "#        (default True)\n",
    "#     \"\"\"\n",
    "#     # assumes detections are in detpath.format(classname)\n",
    "#     # assumes annotations are in annopath.format(imagename)\n",
    "#     # assumes imagesetfile is a text file with each line an image name\n",
    "#     # cachedir caches the annotations in a pickle file\n",
    "#     # first load gt\n",
    "#     if not os.path.isdir(cachedir):\n",
    "#         os.mkdir(cachedir)\n",
    "#     cachefile = os.path.join(cachedir, 'annots.pkl')\n",
    "#     # read list of images\n",
    "#     # with open(imagesetfile, 'r') as f:\n",
    "#     #     lines = f.readlines()\n",
    "#     # imagenames = [x.strip() for x in lines]\n",
    "\n",
    "\n",
    "#     imagenames = []\n",
    "#     listdir = os.listdir(osp.join('%s' % args.SIXray_root, 'Annotation'))\n",
    "#     testdir = listdir[0:6000:5]\n",
    "#     traindir = set(listdir).difference(set(testdir))\n",
    "#     traindir = list(traindir)[0:1200]\n",
    "#     for name in testdir:\n",
    "#         imagenames.append(osp.splitext(name)[0])\n",
    "\n",
    "\n",
    "#     if not os.path.isfile(cachefile):\n",
    "#         # print('not os.path.isfile')\n",
    "#         # load annots\n",
    "#         recs = {}\n",
    "#         for i, imagename in enumerate(imagenames):\n",
    "#             recs[imagename] = parse_rec(annopath % (imagename),imgpath)\n",
    "#             '''\n",
    "#             if i % 100 == 0:\n",
    "#                 print('Reading annotation for {:d}/{:d}'.format(\n",
    "#                     i + 1, len(imagenames)))\n",
    "#             '''\n",
    "#         # save\n",
    "#         # print('Saving cached annotations to {:s}'.format(cachefile))\n",
    "#         with open(cachefile, 'wb') as f:\n",
    "#             pickle.dump(recs, f)\n",
    "#     else:\n",
    "#         # print('no,no,no')\n",
    "#         # load\n",
    "#         with open(cachefile, 'rb') as f:\n",
    "#             recs = pickle.load(f)\n",
    "\n",
    "#     # print (recs)\n",
    "#     # print (classname)\n",
    "\n",
    "#     # extract gt objects for this class\n",
    "#     class_recs = {}\n",
    "#     npos = 0\n",
    "#     for imagename in imagenames:\n",
    "#         R = [obj for obj in recs[imagename] if obj['name'] == classname]\n",
    "\n",
    "#         bbox = np.array([x['bbox'] for x in R])\n",
    "#         difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
    "#         det = [False] * len(R)\n",
    "#         npos = npos + sum(~difficult)\n",
    "#         class_recs[imagename] = {'bbox': bbox,\n",
    "#                                  'difficult': difficult,\n",
    "#                                  'det': det}\n",
    "\n",
    "#     # print (class_recs)\n",
    "\n",
    "#     # read dets\n",
    "#     detfile = detpath.format(classname)\n",
    "#     with open(detfile, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#     if any(lines) == 1:\n",
    "\n",
    "#         splitlines = [x.strip().split(' ') for x in lines]\n",
    "#         image_ids = [x[0] for x in splitlines]\n",
    "#         confidence = np.array([float(x[1]) for x in splitlines])\n",
    "#         BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n",
    "\n",
    "#         # sort by confidence\n",
    "#         sorted_ind = np.argsort(-confidence)\n",
    "#         sorted_scores = np.sort(-confidence)\n",
    "#         BB = BB[sorted_ind, :]\n",
    "#         image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "#         # go down dets and mark TPs and FPs\n",
    "#         nd = len(image_ids)\n",
    "#         tp = np.zeros(nd)\n",
    "#         fp = np.zeros(nd)\n",
    "#         for d in range(nd):\n",
    "#             R = class_recs[image_ids[d]]\n",
    "#             bb = BB[d, :].astype(float)\n",
    "#             ovmax = -np.inf\n",
    "#             BBGT = R['bbox'].astype(float)\n",
    "#             if BBGT.size > 0:\n",
    "#                 # compute overlaps\n",
    "#                 # intersection\n",
    "#                 ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "#                 iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "#                 ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "#                 iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "#                 iw = np.maximum(ixmax - ixmin, 0.)\n",
    "#                 ih = np.maximum(iymax - iymin, 0.)\n",
    "#                 inters = iw * ih\n",
    "#                 uni = ((bb[2] - bb[0]) * (bb[3] - bb[1]) +\n",
    "#                        (BBGT[:, 2] - BBGT[:, 0]) *\n",
    "#                        (BBGT[:, 3] - BBGT[:, 1]) - inters)\n",
    "#                 overlaps = inters / uni\n",
    "#                 ovmax = np.max(overlaps)\n",
    "#                 jmax = np.argmax(overlaps)\n",
    "\n",
    "#             if ovmax > ovthresh:\n",
    "#                 if not R['difficult'][jmax]:\n",
    "#                     if not R['det'][jmax]:\n",
    "#                         tp[d] = 1.\n",
    "#                         R['det'][jmax] = 1\n",
    "#                     else:\n",
    "#                         fp[d] = 1.\n",
    "#             else:\n",
    "#                 fp[d] = 1.\n",
    "\n",
    "#         # compute precision recall\n",
    "#         fp = np.cumsum(fp)\n",
    "#         tp = np.cumsum(tp)\n",
    "#         rec = tp / float(npos)\n",
    "#         # avoid divide by zero in case the first detection matches a difficult\n",
    "#         # ground truth\n",
    "#         prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "#         ap = voc_ap(rec, prec, use_07_metric)\n",
    "#     else:\n",
    "#         rec = -1.\n",
    "#         prec = -1.\n",
    "#         ap = -1.\n",
    "\n",
    "#     return rec, prec, ap\n",
    "\n",
    "\n",
    "# def test_net(save_folder, net, cuda, dataset, transform, top_k,\n",
    "#              im_size=300, thresh=0.05):\n",
    "#     # //\n",
    "#     # //\n",
    "#     num_images = len(dataset)\n",
    "#     # all detections are collected into:\n",
    "#     #    all_boxes[cls][image] = N x 5 array of detections in\n",
    "#     #    (x1, y1, x2, y2, score)\n",
    "#     all_boxes = [[[] for _ in range(num_images)]\n",
    "#                  for _ in range(len(labelmap) + 1)]\n",
    "\n",
    "#     # timers\n",
    "#     _t = {'im_detect': Timer(), 'misc': Timer()}\n",
    "#     output_dir = get_output_dir('ssd300_120000', set_type)\n",
    "#     det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "#     for i in range(num_images):\n",
    "#         im, gt, h, w, og_im = dataset.pull_item(i)\n",
    "#         # 这里im的颜色偏暗，因为BaseTransform减去了一个mean\n",
    "#         # im_saver = cv2.resize(im[(a2,a1,0),:,:].permute((a1,a2,0)).numpy(), (w,h))\n",
    "\n",
    "#         im_det = og_im.copy()\n",
    "#         im_gt = og_im.copy()\n",
    "\n",
    "#         # print(im_det)\n",
    "#         x = Variable(im.unsqueeze(0))\n",
    "#         if args.cuda:\n",
    "#             x = x.cuda()\n",
    "#         _t['im_detect'].tic()\n",
    "#         detections = net(x).data\n",
    "#         detect_time = _t['im_detect'].toc(average=False)\n",
    "\n",
    "#         # skip j = 0, because it's the background class\n",
    "#         # //\n",
    "#         # //\n",
    "#         # print(detections)\n",
    "#         for j in range(1, detections.size(1)):\n",
    "#             dets = detections[0, j, :]\n",
    "#             mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()\n",
    "#             dets = torch.masked_select(dets, mask).view(-1, 5)\n",
    "#             if dets.size(0) == 0:\n",
    "#                 continue\n",
    "#             boxes = dets[:, 1:]\n",
    "#             boxes[:, 0] *= w\n",
    "#             boxes[:, 2] *= w\n",
    "#             boxes[:, 1] *= h\n",
    "#             boxes[:, 3] *= h\n",
    "#             # print(boxes)\n",
    "#             scores = dets[:, 0].cpu().numpy()\n",
    "#             cls_dets = np.hstack((boxes.cpu().numpy(),\n",
    "#                                   scores[:, np.newaxis])).astype(np.float32,\n",
    "#                                                                  copy=False)\n",
    "#             all_boxes[j][i] = cls_dets\n",
    "\n",
    "#             # print(all_boxes)\n",
    "#             for item in cls_dets:\n",
    "#                 # print(item)\n",
    "#                 # print(item[5])\n",
    "#                 if item[4] > thresh:\n",
    "#                     # print(item)\n",
    "#                     chinese = labelmap[j - 1] + str(round(item[4], 2))\n",
    "#                     print(chinese+'det\\n\\n')\n",
    "#                     if chinese[0] == '带':\n",
    "#                         chinese = 'P_Battery_Core' + chinese[6:]\n",
    "#                     else:\n",
    "#                         chinese = 'P_Battery_No_Core' + chinese[7:]\n",
    "#                     cv2.rectangle(im_det, (item[0], item[1]), (item[2], item[3]), (0, 0, 255), 2)\n",
    "#                     cv2.putText(im_det, chinese, (int(item[0]), int(item[1]) - 5), 0,\n",
    "#                                 0.6, (0, 0, 255), 2)\n",
    "#         real = 0\n",
    "#         if gt[0][4] == 3:\n",
    "#             real = 0\n",
    "#         else:\n",
    "#             real = 1\n",
    "\n",
    "#         for item in gt:\n",
    "#             if real == 0:\n",
    "#                 print('this pic dont have the obj:', dataset.ids[i])\n",
    "#                 break\n",
    "#             chinese = labelmap[int(item[4])]\n",
    "#             # print(chinese+'gt\\n\\n')\n",
    "#             if chinese[0] == '带':\n",
    "#                 chinese = 'P_Battery_Core'\n",
    "#             else:\n",
    "#                 chinese = 'P_Battery_No_Core'\n",
    "#             cv2.rectangle(im_det, (int(item[0] * w), int(item[1] * h)), (int(item[2] * w), int(item[3] * h)),\n",
    "#                           (0, 255, 255), 2)\n",
    "#             cv2.putText(im_det, chinese, (int(item[0] * w), int(item[1] * h) - 5), 0, 0.6, (0, 255, 255), 2)\n",
    "#             # print(labelmap[int(item[4])])\n",
    "\n",
    "#         print('im_detect: {:d}/{:d} {:.3f}s'.format(i + 1, num_images, detect_time))\n",
    "\n",
    "#         # cv2.imwrite(f'{SIXray_ROOT}/{dataset.ids[i]}_det.jpg', im_det)\n",
    "#         # cv2.imwrite(f'{SIXray_ROOT}/{dataset.ids[i]}_gt.jpg', im_gt)\n",
    "#         # cv2.imwrite('/media/dsg3/husheng/eval/{0}_det.jpg'.format(dataset.ids[i]), im_det)\n",
    "#         # cv2.imwrite('/media/dsg3/husheng/eval/{0}_gt.jpg'.format(dataset.ids[i]), im_gt)\n",
    "#     #     break\n",
    "#     # return\n",
    "#     #\n",
    "#     with open(det_file, 'wb') as f:\n",
    "#         pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#     # print('Evaluating detections')\n",
    "#     evaluate_detections(all_boxes, output_dir, dataset)\n",
    "\n",
    "\n",
    "# def evaluate_detections(box_list, output_dir, dataset):\n",
    "#     write_voc_results_file(box_list, dataset)\n",
    "#     do_python_eval(output_dir)\n",
    "\n",
    "\n",
    "# def reset_args(EPOCH):\n",
    "#     global args\n",
    "#     root = os.getcwd()\n",
    "#     args.trained_model = \"/content/gdrive/My Drive/weights/sixray_9_1_1000.pth\"\n",
    "#     saver_root = root\n",
    "#     if not os.path.exists(saver_root):\n",
    "#         os.mkdir(saver_root)\n",
    "#     args.save_folder = saver_root + '/test/{:d}epoeich_500/'.format(EPOCH)\n",
    "\n",
    "#     if not os.path.exists(args.save_folder):\n",
    "#         os.mkdir(args.save_folder)\n",
    "#     else:\n",
    "#         shutil.rmtree(args.save_folder)\n",
    "#         os.mkdir(args.save_folder)\n",
    "\n",
    "#     global devkit_path\n",
    "#     devkit_path = args.save_folder\n",
    "\n",
    "\n",
    "# def xavier(param):\n",
    "#     nn.init.xavier_uniform_(param)\n",
    "# def weights_init(m):\n",
    "#     if isinstance(m, nn.Conv2d):\n",
    "#         xavier(m.weight.data)\n",
    "#         m.bias.data.zero_()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # EPOCHS = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80]\n",
    "#     # EPOCHS = [85, 90, 95, 100, 105, 110, 115, 120]\n",
    "#     # EPOCHS = [90, 95, 100, 105, 110, 115, 120, 125]\n",
    "#     EPOCHS = [x for x in range(145, 205, 5)]\n",
    "#     print(EPOCHS)\n",
    "#     for EPOCH in EPOCHS:\n",
    "#         reset_args(EPOCH)\n",
    "\n",
    "\n",
    "#         # load net\n",
    "#         num_classes = len(labelmap) + 1  # +a1 for background\n",
    "#         cfg = voc\n",
    "#         cfg['num_classes'] = 3\n",
    "#         net = build_ssd('test', 300, num_classes)  # initialize SSD\n",
    "#         # from ssd_net_vgg import SSD\n",
    "#         # net = SSD()\n",
    "#         # net.apply(weights_init)\n",
    "#         # print(dir(net))\n",
    "#         # print(net.state_dict)\n",
    "#         print(args.trained_model)\n",
    "#         # print(torch.load(args.trained_model))\n",
    "#         net.load_state_dict(torch.load(args.trained_model))\n",
    "#         net.eval()\n",
    "\n",
    "#         # print('Finished loading model!')\n",
    "#         # load data\n",
    "#         dataset = SIXrayDetection(args.SIXray_root, args.imagesetfile,\n",
    "#                                   BaseTransform(300, dataset_mean),\n",
    "#                                   SIXrayAnnotationTransform(), phase='test')\n",
    "#         print(args)\n",
    "#         # args.cuda = False\n",
    "#         if args.cuda:\n",
    "#             net = net.cuda()\n",
    "#             cudnn.benchmark = True\n",
    "#         # evaluation\n",
    "\n",
    "#         # with open(\"./ssd300_120000/test/detections.pkl\", 'rb') as fo:  # 读取pkl文件数据\n",
    "#         #     all_boxes = pickle.load(fo, encoding='bytes')\n",
    "#         # output_dir = get_output_dir('ssd300_120000', set_type)\n",
    "#         # evaluate_detections(all_boxes, output_dir, dataset)\n",
    "\n",
    "#         test_net(args.save_folder, net, args.cuda, dataset,\n",
    "#                  BaseTransform(net.size, dataset_mean), args.top_k, 300,\n",
    "#                  thresh=args.confidence_threshold)\n",
    "#         # if EPOCH == 150:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Z4-09zh8awu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v18ywwiu6nW6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ssd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
